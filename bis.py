# -*- coding: utf-8 -*-
"""BIS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zVHXK2NUCQX_17uyGfESfLdVeYa4jUju
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install pm4py

import numpy as np
from pm4py.objects.log.importer.xes import importer as xes_importer
from pm4py.objects.conversion.log import converter as log_converter
import pandas as pd


DomesticDeclarations = './DomesticDeclarations.xes'
InternationalDeclarations = './InternationalDeclarations.xes'
PermitLog = './PermitLog.xes'
PrepaidTravelCost = './PrepaidTravelCost.xes'
RequestForPayment = './RequestForPayment.xes'

list_xes = [DomesticDeclarations, InternationalDeclarations, PermitLog, PrepaidTravelCost, RequestForPayment]
list_name = ["DomesticDeclarations", "InternationalDeclarations", "PermitLog", "PrepaidTravelCost", "RequestForPayment"]



def get_logs(xes_file):
    return xes_importer.apply(xes_file)

def log_to_df(log):
    return log_converter.apply(log, variant=log_converter.Variants.TO_DATA_FRAME)

def df_to_log(df):
    return log_converter.apply(df, variant=log_converter.Variants.TO_EVENT_LOG)

data_log = {n:get_logs(f) for n,f in zip(list_name, list_xes)}

data_log_from_2017 = {k:len(data_log[k]) for k in data_log.keys()}
data_log_from_2017

data_df = {k:log_to_df(data_log[k]) for k in data_log.keys()}

"""**Temporal filtering - 2018**"""

from pm4py.algo.filtering.log.timestamp import timestamp_filter

data_log_from_2018 = {
    k:timestamp_filter.filter_traces_contained(data_df[k], "2018-01-01 00:00:00", "2022-07-01 23:59:59",
                parameters={timestamp_filter.Parameters.TIMESTAMP_KEY: "time:timestamp"}) for k in data_df.keys()
                }

data_log_from_2018_num = {k:len(data_log_from_2018[k]) for k in data_log_from_2018.keys()}
data_log_from_2018_num

"""**Filtering sulle top k vairanti**"""

from pm4py.statistics.traces.generic.log import case_statistics
variants_count = case_statistics.get_variant_statistics(data_log['DomesticDeclarations'])
variants_count = sorted(variants_count, key=lambda x: x['count'], reverse=True)

variants_df = pd.DataFrame.from_records(variants_count)
variants_df.head()

import numpy as np
import matplotlib.pyplot as plt
import math
from scipy.stats import pareto
variant = variants_df[0:100].index
frequency = variants_df[0:100]['count']
# return a list of log values from a list 
frequency_log = [math.log(i, 2) for i in frequency] 

# Plot the histogram of the frequencies
fig = plt.figure(figsize = (15, 5))
 
# creating the bar plot
plt.bar(variant, frequency, color ='orange',
        width = 0.4)
 
plt.xlabel("variants sorted by frequency")
plt.ylabel("frequency")
plt.title("bar chart of variants frequency")
plt.show()

import pm4py
log_top = {k:pm4py.filter_variants_top_k(data_log[k], 6) for k in data_log.keys()}

for k,v in log_top.items():
    print (f"{k} --> {round(100*(len(v)/len(data_log[k])))}% - {len(v)}/{len(data_log[k])}")

"""**Distribuzione nel tempo - non nel report perchè non troppo utile**"""

#DomesticDeclarations
from pm4py.algo.filtering.log.attributes import attributes_filter
x, y = attributes_filter.get_kde_date_attribute(data_log['DomesticDeclarations'], attribute="time:timestamp")
from pm4py.visualization.graphs import visualizer as graphs_visualizer
gviz = graphs_visualizer.apply_plot(x, y, variant=graphs_visualizer.Variants.DATES)
graphs_visualizer.view(gviz)

#International Declarations
from pm4py.algo.filtering.log.attributes import attributes_filter
x, y = attributes_filter.get_kde_date_attribute(data_log['InternationalDeclarations'], attribute="time:timestamp")
from pm4py.visualization.graphs import visualizer as graphs_visualizer
gviz = graphs_visualizer.apply_plot(x, y, variant=graphs_visualizer.Variants.DATES)
graphs_visualizer.view(gviz)

#DomesticDeclarations
from pm4py.algo.filtering.log.attributes import attributes_filter
x, y = attributes_filter.get_kde_date_attribute(data_log['PermitLog'], attribute="time:timestamp")
from pm4py.visualization.graphs import visualizer as graphs_visualizer
gviz = graphs_visualizer.apply_plot(x, y, variant=graphs_visualizer.Variants.DATES)
graphs_visualizer.view(gviz)

##idea: mostrare quando le persone fanno la richiesta o quando essettivamente vengono pagate

pm4py.view_events_distribution_graph(data_log_from_2018['DomesticDeclarations'], distr_type="days_week")
pm4py.view_events_distribution_graph(data_log_from_2018['DomesticDeclarations'], distr_type="months")

pm4py.view_events_distribution_graph(data_log_from_2018['InternationalDeclarations'], distr_type="days_week")
pm4py.view_events_distribution_graph(data_log_from_2018['InternationalDeclarations'], distr_type="months")

"""**Calcolo del Process Model and Process Map - Iniziali**"""

process_tree = pm4py.discover_tree_inductive(log_to_df(data_log['DomesticDeclarations']))
bpmn_model = pm4py.convert_to_bpmn(process_tree)
pm4py.view_bpmn(bpmn_model)

dfg, start_activities, end_activities = pm4py.discover_dfg(log_to_df(data_log['DomesticDeclarations']))
pm4py.view_dfg(dfg, start_activities, end_activities)

process_tree = pm4py.discover_tree_inductive(log_to_df(data_log['InternationalDeclarations']))
bpmn_model = pm4py.convert_to_bpmn(process_tree)
pm4py.view_bpmn(bpmn_model)

dfg, start_activities, end_activities = pm4py.discover_dfg(log_to_df(data_log['InternationalDeclarations']))
pm4py.view_dfg(dfg, start_activities, end_activities)

## Import the dfg_discovery algorithm
from pm4py.algo.discovery.dfg import algorithm as dfg_discovery
## Import the dfg visualization object
from pm4py.visualization.dfg import visualizer as dfg_visualization
#Create graph from log
dfg = dfg_discovery.apply(data_log['DomesticDeclarations'])
# Visualise
gviz = dfg_visualization.apply(dfg, log=data_log['DomesticDeclarations'], variant=dfg_visualization.Variants.FREQUENCY)
dfg_visualization.view(gviz)

"""**Durata media totale**"""

import pm4py
all_case_durations = {
    k:pm4py.get_all_case_durations(data_log_from_2018[k]) for k in data_df.keys()
                }
max_case_durations = {
    k:sum(all_case_durations[k])/len(all_case_durations[k]) for k in data_df.keys()
                }

max_case_durations

"""**Durata media del tempo lead per ogni fase da Declaration SUBMITTED by EMPLOYEE a Payment Hangled**"""

#prendere solo gli eventi tra "Declaration SUBMITTED by EMPLOYEE", "Payment Handled"
data_df = log_to_df(data_log_from_2018["DomesticDeclarations"])
filtered_log = pm4py.filter_between(data_df, "Declaration SUBMITTED by EMPLOYEE", "Payment Handled")

from pm4py.objects.log.util import interval_lifecycle

enriched_log = interval_lifecycle.assign_lead_cycle_time(df_to_log(filtered_log))
enriched_log
data_df = log_to_df(enriched_log)
df_new = data_df[['concept:name', '@@approx_bh_overall_wasted_time']]
df_new.groupby('concept:name', as_index=False)['@@approx_bh_overall_wasted_time'].mean()
#df_new = df_new.groupby('concept:name', as_index=False)['@@approx_bh_overall_wasted_time'].mean()
#df_new['days'] = pd.to_timedelta(df_new['@@approx_bh_overall_wasted_time'], unit='D')

#prendere solo gli eventi tra "Declaration SUBMITTED by EMPLOYEE", "Payment Handled"
from pm4py.objects.log.util import interval_lifecycle
import pm4py
data_df = log_to_df(data_log_from_2018["InternationalDeclarations"])
filtered_log = pm4py.filter_between(data_df, "Declaration SUBMITTED by EMPLOYEE", "Payment Handled")

enriched_log = interval_lifecycle.assign_lead_cycle_time(df_to_log(filtered_log))
enriched_log
data_df = log_to_df(enriched_log)
df_new = data_df[['concept:name', '@@approx_bh_overall_wasted_time']]
df_new.groupby('concept:name', as_index=False)['@@approx_bh_overall_wasted_time'].mean()
#df_new = df_new.groupby('concept:name', as_index=False)['@@approx_bh_overall_wasted_time'].mean()
#df_new['days'] = pd.to_timedelta(df_new['@@approx_bh_overall_wasted_time'], unit='D')

"""**Durata media tra diverse fasi dei viaggi domestic e international**"""

data_df = log_to_df(data_log_from_2018["DomesticDeclarations"])
#data_df = log_to_df(data_log_from_2018["InternationalDeclarations"])
filtered_log = pm4py.filter_between(data_df, "Declaration SUBMITTED by EMPLOYEE", "Payment Handled")
m = pm4py.get_all_case_durations(df_to_log(filtered_log))
print((sum(m)/len(m)) / 60 /60 / 24)

print(1004347.538909275 / 60 /60 / 24)

#data_df = log_to_df(data_log_from_2018["DomesticDeclarations"])
data_df = log_to_df(data_log_from_2018["InternationalDeclarations"])
filtered_log = pm4py.filter_between(data_df, "Request Payment", "Payment Handled")
m = pm4py.get_all_case_durations(df_to_log(filtered_log))
print((sum(m)/len(m)) / 60 /60 / 24)

#data_df = log_to_df(data_log_from_2018["DomesticDeclarations"])
data_df = log_to_df(data_log_from_2018["InternationalDeclarations"])
filtered_log = pm4py.filter_between(data_df, "Declaration FINAL_APPROVED by SUPERVISOR", "Request Payment")
m = pm4py.get_all_case_durations(df_to_log(filtered_log))
print((sum(m)/len(m)) / 60 /60 / 24)

"""**Vedere il Bottlenecks mediante le differenze di giorni come mostrato sopra**"""

for k in data_log.keys():
    list_res = []
    for trace in data_log[k]:
        start = None
        end = None
        for event in trace:
            if event['concept:name'] == 'Declaration SUBMITTED by EMPLOYEE':
                start = event['time:timestamp']
            elif event['concept:name'] == 'Payment Handled':
                end = event['time:timestamp']
                break
        if start is not None and end is not None:
            list_res.append((start, end, trace.attributes['concept:name']))

    print(k,"\n", pd.Series([(end-start).total_seconds()/60/60/24 for (start, end, _) in list_res]).describe()[["25%","50%", "75%"]])

# From Request Payment to Payment Handled
for k in data_log.keys():
    list_res = []
    for trace in data_log[k]:
        approve_event = None
        request_event = None
        for event in trace:
            if event['concept:name'] == 'Request Payment':
                approve_event = event['time:timestamp']
            elif event['concept:name'] == 'Payment Handled':
                request_event = event['time:timestamp']
                break
        if request_event is not None and approve_event is not None:
            list_res.append((approve_event, request_event, trace.attributes['concept:name']))
    #print(k,"\n", pd.Series([(end-start).total_seconds()/60/60/24 for (start, end, _) in list_res]).describe()[["mean"]])
    print(k,"\n", pd.Series([(end-start).total_seconds()/60/60/24 for (start, end, _) in list_res]).describe()[["25%","50%", "75%"]])

# From Declaration FINAL_APPROVED by SUPERVISOR to Request Payment
for k in data_log.keys():
    list_res = []
    for trace in data_log[k]:
        approve_event = None
        request_event = None
        for event in trace:
            if event['concept:name'] == 'Declaration FINAL_APPROVED by SUPERVISOR':
                approve_event = event['time:timestamp']
            elif event['concept:name'] == 'Request Payment':
                request_event = event['time:timestamp']
                break
        if request_event is not None and approve_event is not None:
            list_res.append((approve_event, request_event, trace.attributes['concept:name']))
    print(k,"\n", pd.Series([(end-start).total_seconds()/60/60/24 for (start, end, _) in list_res]).describe()[["25%","50%", "75%", "max"]])

"""**Doppi pagamenti**"""

#Con pandas non ci sono riuscito
data_df = log_to_df(data_log["PermitLog"])
df_new.groupby('declaration_id', as_index=False).apply(lambda g: g['concept:name'] == 'Payment Handled').count()
data_df

#con i log seguenzialmente

dict_decl = set()
double_time = []
for trace in data_log["PermitLog"]:
  for event in trace:
    if event['concept:name'] == 'Payment Handled':
      dec = trace.attributes['concept:name']
      if dec in dict_decl:
        double_time.append(dec)
      else:
        dict_decl.add(dec)

#print(data_log["PermitLog"][0])
set_double = list(set(double_time))
res_dict = {k:[] for k in set_double}
for trace in data_log["PermitLog"]:
    if (trace.attributes["concept:name"]) in set_double:
        for event in trace:
            if event['concept:name'] == 'Payment Handled':
                res_dict[trace.attributes["concept:name"]].append(trace.attributes["TotalDeclared"])
                #ogni quando veniva rilasciato un altro pagamento per poi fare una media - poco utile
                #res_dict[trace.attributes["concept:name"]].append(trace.attributes["time:timestamp"])
tmp = []
#print(res_dict)
for k, v in res_dict.items():
    tmp.append([k, round(v[0]), len(v)])
#print(tmp)
df_res = pd.DataFrame(tmp, columns=["ID", "AMOUNT", "TIMES"])
df_res["LOSS"] = df_res["AMOUNT"].to_numpy()*(df_res["TIMES"].to_numpy()-1)
df_res.sort_values("LOSS", ascending=False).head(10)

df_res['LOSS'].sum()

"""**Trovare un buon modello**"""

from pm4py.algo.evaluation.replay_fitness import algorithm as replay_fitness_evaluator
from pm4py.algo.evaluation.precision import algorithm as precision_evaluator
from pm4py.algo.evaluation.generalization import algorithm as generalization_evaluator
from pm4py.algo.evaluation.simplicity import algorithm as simplicity_evaluator
# viz
from pm4py.visualization.petri_net import visualizer as pn_visualizer
from pm4py.visualization.process_tree import visualizer as pt_visualizer
from pm4py.visualization.dfg import visualizer as dfg_visualization
# process mining 
from pm4py.algo.discovery.alpha import algorithm as alpha_miner
from pm4py.algo.discovery.inductive import algorithm as inductive_miner
from pm4py.algo.discovery.heuristics import algorithm as heuristics_miner
from pm4py.algo.discovery.dfg import algorithm as dfg_discovery
from pm4py.algo.discovery.batches import algorithm 


# misc 
from pm4py.objects.conversion.process_tree import converter as pt_converter

def score(net, im, fm, log):
    fitness = pm4py.fitness_token_based_replay(log, net, im, fm)
    prec = pm4py.precision_token_based_replay(log, net, im, fm)
    gen = generalization_evaluator.apply(log, net, im, fm)
    simp = simplicity_evaluator.apply(net)
    return round(fitness['log_fitness'],2), round(prec,2), round(gen,2), round(simp,2)

def model_stats(log):
    tmp=[]
    #Alpha
    net, im, fm = pm4py.discover_petri_net_alpha(log)
    f,p,g,s = score(net, im, fm, log)
    tmp.append(["Alpha", ["NONE"], f,p,g,s])
    #Inductive
    net, im, fm = pm4py.discover_petri_net_inductive(log)
    f,p,g,s = score(net, im, fm, log)
    tmp.append(["IM", ["NONE"], f,p,g,s])
    #Inductive Miner directly-follows
    net, im, fm = inductive_miner.apply(log, variant=inductive_miner.Variants.IMd)
    f,p,g,s = score(net, im, fm, log)
    tmp.append(["IMd", ["NONE"], f,p,g,s])

    for n in range(0, 11):
      NOISE=n/10 
      net, im, fm = inductive_miner.apply(log, variant=inductive_miner.Variants.IMf, parameters={inductive_miner.Variants.IMf:NOISE})
      f,p,g,s = score(net, im, fm, log)
      tmp.append(["IMf", [NOISE], f,p,g,s])

    for dep in range(1, 11,1):
      net, im, fm = heuristics_miner.apply(log, parameters={heuristics_miner.Variants.CLASSIC.value.Parameters.DEPENDENCY_THRESH: dep/10})
      f,p,g,s = score(net, im, fm, log)
      tmp.append(["HEU", dep/10, f,p,g,s])

    return pd.DataFrame(tmp, columns=["MODEL", "PARAMETERS", "FITNESS", "PRECISION", "GENERALIZATION", "SEMPLICITY"])

import pm4py
d_log_var = {k:pm4py.filter_variants_top_k(data_log[k], 5) for k in data_log.keys()}

"""**DomesticDeclarations**"""

res = model_stats(d_log_var['DomesticDeclarations'])

res['SUM'] = res['FITNESS'] + res['PRECISION'] 
res.sort_values(['SUM',"SEMPLICITY", "GENERALIZATION"], ascending=False).head(5)

from pm4py.objects.conversion.process_tree import converter
from pm4py.objects.conversion.wf_net import converter as wf_net_converter
from pm4py.visualization.heuristics_net import visualizer as hn_visualizer

# Discover process tree using inductive miner
net, im, fm = heuristics_miner.apply(d_log_var['DomesticDeclarations'], parameters={heuristics_miner.Variants.CLASSIC.value.Parameters.DEPENDENCY_THRESH: 0.1})
# Visualise
parameters = {pn_visualizer.Variants.FREQUENCY.value.Parameters.FORMAT: "png"}
gviz = pn_visualizer.apply(net, im, fm, parameters=parameters, variant=pn_visualizer.Variants.FREQUENCY, log=d_log_var['DomesticDeclarations'])
pn_visualizer.view(gviz)

heu_net = pm4py.discover_heuristics_net(d_log_var['DomesticDeclarations'], dependency_threshold=0.1)
pm4py.view_heuristics_net(heu_net)

"""**InternationalDeclarations**"""

res = model_stats(d_log_var['InternationalDeclarations'])

res['SUM'] = res['FITNESS'] + res['PRECISION'] 
res.sort_values(['SUM',"SEMPLICITY", "GENERALIZATION"], ascending=False).head(5)

from pm4py.objects.conversion.process_tree import converter
from pm4py.objects.conversion.wf_net import converter as wf_net_converter
from pm4py.visualization.heuristics_net import visualizer as hn_visualizer

# Discover process tree using inductive miner
net, im, fm = heuristics_miner.apply(d_log_var['InternationalDeclarations'], parameters={heuristics_miner.Variants.CLASSIC.value.Parameters.DEPENDENCY_THRESH: 0.1})
# Visualise
parameters = {pn_visualizer.Variants.FREQUENCY.value.Parameters.FORMAT: "png"}
gviz = pn_visualizer.apply(net, im, fm, parameters=parameters, variant=pn_visualizer.Variants.FREQUENCY, log=d_log_var['DomesticDeclarations'])
pn_visualizer.view(gviz)

heu_net = pm4py.discover_heuristics_net(d_log_var['InternationalDeclarations'], dependency_threshold=0.1)
pm4py.view_heuristics_net(heu_net)

"""**PermitLog**"""

res = model_stats(d_log_var['PermitLog'])

res['SUM'] = res['FITNESS'] + res['PRECISION'] 
res.sort_values(['SUM',"SEMPLICITY", "GENERALIZATION"], ascending=False).head(5)

# Discover process tree using inductive miner
net, im, fm = heuristics_miner.apply(d_log_var['PermitLog'], parameters={heuristics_miner.Variants.CLASSIC.value.Parameters.DEPENDENCY_THRESH: 0.1})
# Visualise
parameters = {pn_visualizer.Variants.FREQUENCY.value.Parameters.FORMAT: "png"}
gviz = pn_visualizer.apply(net, im, fm, parameters=parameters, variant=pn_visualizer.Variants.FREQUENCY, log=d_log_var['DomesticDeclarations'])
pn_visualizer.view(gviz)

heu_net = pm4py.discover_heuristics_net(d_log_var['PermitLog'], dependency_threshold=0.1)
pm4py.view_heuristics_net(heu_net)

"""**RequestForPayment**"""

res = model_stats(d_log_var['RequestForPayment'])

res['SUM'] = res['FITNESS'] + res['PRECISION'] 
res.sort_values(['SUM',"SEMPLICITY", "GENERALIZATION"], ascending=False).head(5

# Discover process tree using inductive miner
net, im, fm = heuristics_miner.apply(d_log_var['RequestForPayment'], parameters={heuristics_miner.Variants.CLASSIC.value.Parameters.DEPENDENCY_THRESH: 0.1})
# Visualise
parameters = {pn_visualizer.Variants.FREQUENCY.value.Parameters.FORMAT: "png"}
gviz = pn_visualizer.apply(net, im, fm, parameters=parameters, variant=pn_visualizer.Variants.FREQUENCY, log=d_log_var['DomesticDeclarations'])
pn_visualizer.view(gviz)

heu_net = pm4py.discover_heuristics_net(d_log_var['RequestForPayment'], dependency_threshold=0.1)
pm4py.view_heuristics_net(heu_net)

"""**Prepaid Travel Cost**"""

res = model_stats(d_log_var['PrepaidTravelCost'])

res['SUM'] = res['FITNESS'] + res['PRECISION'] 
res.sort_values(['SUM',"SEMPLICITY", "GENERALIZATION"], ascending=False).head(5)

net, im, fm = inductive_miner.apply(d_log_var["PrepaidTravelCost"], variant=inductive_miner.Variants.IMf, parameters={inductive_miner.Variants.IMf:0.0})

from pm4py.visualization.petri_net import visualizer as pn_visualizer
parameters = {pn_visualizer.Variants.FREQUENCY.value.Parameters.FORMAT: "png"}
gviz = pn_visualizer.apply(net,im, fm, parameters=parameters, variant=pn_visualizer.Variants.FREQUENCY, log=d_log_var['PrepaidTravelCost'])
pn_visualizer.view(gviz)

"""**CONFORMANCE CHECKING**"""

def check(log, net, initial_marking, final_marking):
  replayed_traces_token = pm4py.conformance_diagnostics_token_based_replay(log, net, initial_marking, final_marking)
  aligned_traces = pm4py.conformance_diagnostics_alignments(log, net, initial_marking, final_marking)

  df_token = pd.DataFrame(replayed_traces_token)
  df_align = pd.DataFrame(aligned_traces)

  print("REPLAY TRACES TOKEN")
  print("Numeber of traces: " , df_token['trace_is_fit'].count())
  print("Numeber of traces fit: ", df_token['trace_is_fit'].value_counts()[True])
  print("Numeber of anomalous traces: ", 
        (df_token['trace_is_fit'].count() - df_token['trace_is_fit'].value_counts()[True]))
  print("Percentage of anomalous traces: ", 
        ((df_token['trace_is_fit'].count() - df_token['trace_is_fit'].value_counts()[True]) / df_token['trace_is_fit'].count()) * 100)
  print("ALIGNED")
  print("Numeber of trace: " , df_align['fitness'].count())
  print("Numeber of traces fit: ", df_align['fitness'].value_counts()[1])
  print("Numeber of anomalous traces: ", 
        (df_align['fitness'].count() - df_align['fitness'].value_counts()[1]))
  print("Percentage of anomalous traces: ", 
        ((df_align['fitness'].count() - df_align['fitness'].value_counts()[1]) /df_align['fitness'].count()) * 100)

"""Viaggi nazionali"""

net, initial_marking, final_marking = heuristics_miner.apply(d_log_var["DomesticDeclarations"], parameters={heuristics_miner.Variants.CLASSIC.value.Parameters.DEPENDENCY_THRESH: 0.1})

check(d_log_var["DomesticDeclarations"],net, initial_marking, final_marking )

net, initial_marking, final_marking = net, im, fm = heuristics_miner.apply(d_log_var["InternationalDeclarations"], variant=inductive_miner.Variants.IMf, parameters={inductive_miner.Variants.IMf:0.0})
      

check(d_log_var["InternationalDeclarations"],net, initial_marking, final_marking )

net, initial_marking, final_marking = net, im, fm = heuristics_miner.apply(d_log_var["PermitLog"], variant=inductive_miner.Variants.IMf, parameters={inductive_miner.Variants.IMf:0.0})
      

check(d_log_var["PermitLog"],net, initial_marking, final_marking )

net, initial_marking, final_marking = net, im, fm = heuristics_miner.apply(d_log_var["RequestForPayment"], variant=inductive_miner.Variants.IMf, parameters={inductive_miner.Variants.IMf:0.0})
      

check(d_log_var["RequestForPayment"],net, initial_marking, final_marking )

net, initial_marking, final_marking = net, im, fm = inductive_miner.apply(d_log_var["PrepaidTravelCost"], variant=inductive_miner.Variants.IMf, parameters={inductive_miner.Variants.IMf:0.0})
      

check(d_log_var["PrepaidTravelCost"],net, initial_marking, final_marking )

"""**Provare a predirre il costo dei viaggi**"""

#solo 2018 in poi. risultati migliori
data_df = {k:log_to_df(data_log_from_2018[k]) for k in ['InternationalDeclarations','PermitLog']}

#from pm4py.algo.simulation.playout.petri_net import algorithm as simulator
#net, im, fn = heuristics_miner.apply(d_log_var['InternationalDeclarations'], parameters={heuristics_miner.Variants.CLASSIC.value.Parameters.DEPENDENCY_THRESH: 0.1})

#simulated_log = simulator.apply(net, im, variant=simulator.Variants.BASIC_PLAYOUT, parameters={simulator.Variants.BASIC_PLAYOUT.value.Parameters.ACTIVITY_KEY: [data_df["InternationalDeclarations"].keys()]})
#print(len(simulated_log))
#simulated_df = log_to_df(simulated_log)

#simulated_df
data_df["InternationalDeclarations"]

from pm4py.algo.transformation.log_to_features import algorithm as log_to_features
#print(len(data_df["InternationalDeclarations"]))
#result = pd.concat([data_df["InternationalDeclarations"], simulated_df])
#print(len(result)-len(data_df["InternationalDeclarations"]))
#Automatic Feature Selection 
data, feature_names = log_to_features.apply(data_df["InternationalDeclarations"])
print(feature_names)

flat_international = data_df["InternationalDeclarations"][(data_df["InternationalDeclarations"]["case:concept:name"].notna()) & (data_df["InternationalDeclarations"]["case:concept:name"] != "UNKNOWN")].sort_values("time:timestamp").groupby("case:concept:name").nth((0,-1)).reset_index()[["case:concept:name","case:Amount", "case:Permit RequestedBudget","case:AdjustedAmount", "case:Permit ID"]].drop_duplicates()
flat_international.head()

diff_trav = data_df["PermitLog"][data_df["PermitLog"]["concept:name"].isin(["Start trip", "End trip"])].sort_values("time:timestamp").groupby("case:concept:name").nth((0,-1)).groupby("case:concept:name")["time:timestamp"].diff().dropna().map(lambda x: (x.days))

flat_permit = data_df["PermitLog"][(data_df["PermitLog"]["case:concept:name"].notna()) & (data_df["PermitLog"]["case:concept:name"] != "UNKNOWN")].sort_values("time:timestamp").groupby("case:concept:name").nth((-1)).reset_index()[["case:concept:name","case:Overspent", "case:RequestedBudget","case:OverspentAmount", "case:TotalDeclared"]]#.drop_duplicates()
flat_permit = flat_permit.merge(diff_trav, on="case:concept:name").drop_duplicates("case:concept:name")
flat_permit.rename(columns = {'case:concept:name':'case:Permit ID'}, inplace = True)

merged = flat_permit.merge(flat_international, on="case:Permit ID")
#merged

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay

from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB

cols_predictor = ['case:RequestedBudget',
       'time:timestamp','case:Permit RequestedBudget', 'case:TotalDeclared', 'case:Amount', 'case:Overspent']

X = merged.iloc[:, [2,4,5,7,8,9]].values
y = merged.iloc[:, 1].values

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=0, stratify=y)

# Feature Scaling
#from sklearn.preprocessing import StandardScaler

#sc = StandardScaler()
#X_train = sc.fit_transform(x_train)
#X_test = sc.transform(x_train)

from sklearn import metrics
knn = KNeighborsClassifier()
bayees = GaussianNB()
rf = RandomForestClassifier()

ms = [knn,bayees,rf]
for m in ms:
    print(m , '\n')
    m.fit(x_train,y_train)
    y_pred = m.predict(x_test)
    print(accuracy_score(y_train, m.predict(x_train)))
    print(accuracy_score(y_test, y_pred))
    print(confusion_matrix(y_test, y_pred))

    print("-"*10 , '\n')

from sklearn.decomposition import PCA


rf = RandomForestClassifier()
for c in range(1,len(cols_predictor)):
    print(c , '\n')
    pca = PCA(n_components=c)
    x_train_pca = pca.fit_transform(x_train)
    x_test_pca = pca.fit_transform(x_test)
    rf.fit(x_train_pca, y_train)
    print(accuracy_score(y_train, rf.predict(x_train_pca)))
    print(accuracy_score(y_test, rf.predict(x_test_pca)))
    print(confusion_matrix(y_test, rf.predict(x_test_pca)))

    print("-"*10)

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

rf = RandomForestClassifier()

#pca = PCA(n_components=5)
#x_train_pca = pca.fit_transform(x_train)
#x_test_pca = pca.fit_transform(x_test)

rf.fit(x_train, y_train)
y_pred = rf.predict(x_test)

print("Test accuracy: ",accuracy_score(y_test, rf.predict(x_test)))
# View the classification report for test data and predictions
print(classification_report(y_test, y_pred))
#print('Mean Absolute Error (MAE):', metrics.mean_absolute_error(y_test, y_pred))
#print('Mean Squared Error (MSE):', metrics.mean_squared_error(y_test, y_pred))

ConfusionMatrixDisplay(confusion_matrix(y_test, rf.predict(x_test))).plot()

# Get and reshape confusion matrix data
matrix = confusion_matrix(y_test, y_pred)
matrix = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]

# Build the plot
plt.figure(figsize=(16,7))
sns.set(font_scale=1.4)
sns.heatmap(matrix, annot=True, annot_kws={'size':10},
            cmap=plt.cm.Greens, linewidths=0.2)

# Add labels to the plot
class_names = ['0', '1']
tick_marks = np.arange(len(class_names))
tick_marks2 = tick_marks + 0.5
plt.xticks(tick_marks, class_names, rotation=25)
plt.yticks(tick_marks2, class_names, rotation=0)
plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.title('Confusion Matrix for Random Forest Model')
plt.show()

#bayees = GaussianNB()
#bayees.fit(x_train_pca, y_train)
#print("Test accuracy: ",accuracy_score(y_test, bayees.predict(x_test_pca)))
#ConfusionMatrixDisplay(confusion_matrix(y_test, bayees.predict(x_test_pca))).plot()

"""**Prove con risultati non troppo utili**

**Clustering (SNA results)**
"""

import pm4py
sa_metric = pm4py.discover_activity_based_resource_similarity(data_log_from_2018['DomesticDeclarations'])

from pm4py.algo.organizational_mining.sna import util
clustering = util.cluster_affinity_propagation(sa_metric)

"""**Playout of a DFG**"""

dfg, sa, ea = pm4py.discover_directly_follows_graph(data_log_from_2018['DomesticDeclarations'])
activities_count = pm4py.get_event_attribute_values( data_log_from_2018['DomesticDeclarations'], "concept:name")
activities_count

#Simulation (Utile per creare log per allenare il modello predittivo)
from pm4py.algo.simulation.playout.petri_net import algorithm as simulator
net, im, fn = heuristics_miner.apply(d_log_var['DomesticDeclarations'], parameters={heuristics_miner.Variants.CLASSIC.value.Parameters.DEPENDENCY_THRESH: 0.1})

simulated_log = simulator.apply(net, im, variant=simulator.Variants.BASIC_PLAYOUT, parameters={simulator.Variants.BASIC_PLAYOUT.value.Parameters.NO_TRACES: 50})
print(simulated_log)

#Clustering 
sa_metric = pm4py.discover_activity_based_resource_similarity(d_log_var['DomesticDeclarations'])

from pm4py.algo.organizational_mining.sna import util
clustering = util.cluster_affinity_propagation(sa_metric)
print(clustering)

"""**Feature Selection - An operation of feature selection permits to represent the event log in a tabular way. This is important for operations such as prediction and anomaly detection.**"""

#Automatic Feature Selection
from pm4py.algo.transformation.log_to_features import algorithm as log_to_features

data, feature_names = log_to_features.apply(data_log['DomesticDeclarations'])
print(feature_names)
import pandas as pd
df = pd.DataFrame(data, columns=feature_names)
print(df)

data, feature_names = log_to_features.apply(data_log['InternationalDeclarations'])
print(feature_names)
import pandas as pd
df = pd.DataFrame(data, columns=feature_names)
print(df)

#PCA – Reducing the number of features
from sklearn.decomposition import PCA

pca = PCA(n_components=5)
df2 = pd.DataFrame(pca.fit_transform(df))
#Anomaly Detection
from sklearn.ensemble import IsolationForest
model=IsolationForest()
model.fit(df2)
df2["scores"] = model.decision_function(df2)
                     
df2["@@index"] = df2.index
df2 = df2[["scores", "@@index"]]
df2 = df2.sort_values("scores")
print(df2)

"""**Evolution of the Features**"""

import pm4py
from pm4py.algo.transformation.log_to_features.util import locally_linear_embedding
from pm4py.visualization.graphs import visualizer

x, y = locally_linear_embedding.apply(data_log['DomesticDeclarations'])
gviz = visualizer.apply(x, y, variant=visualizer.Variants.DATES,
                        parameters={"title": "Locally Linear Embedding", "format": "svg", "y_axis": "Intensity"})
visualizer.view(gviz)

## Import the alpha_miner algorithm
from pm4py.algo.discovery.alpha import algorithm as alpha_miner
net, initial_marking, final_marking = alpha_miner.apply(data_log['DomesticDeclarations'])

## Import the petrinet visualizer object
from pm4py.visualization.petrinet import visualizer as pn_visualizer
# Visualise 
gviz = pn_visualizer.apply(net, initial_marking, final_marking, variant=pn_visualizer.Variants.FREQUENCY, log=data_log['DomesticDeclarations'])
pn_visualizer.view(gviz)

from pm4py.algo.discovery.inductive import algorithm as inductive_miner
# Discover process tree using inductive miner
tree = inductive_miner.apply_tree(data_log['DomesticDeclarations'])

from pm4py.visualization.process_tree import visualizer as pt_visualizer
gviz = pt_visualizer.apply(tree, parameters={pt_visualizer.Variants.WO_DECORATION.value.Parameters.FORMAT: "png"})
pt_visualizer.view(gviz)

## Import the dfg_discovery algorithm
from pm4py.algo.discovery.dfg import algorithm as dfg_discovery
## Import the dfg visualization object
from pm4py.visualization.dfg import visualizer as dfg_visualization
#Create graph from log
dfg = dfg_discovery.apply(data_log['DomesticDeclarations'])
# Visualise
gviz = dfg_visualization.apply(dfg, log=data_log['DomesticDeclarations'], variant=dfg_visualization.Variants.FREQUENCY)
dfg_visualization.view(gviz)

## Import the alpha_miner algorithm
from pm4py.algo.discovery.alpha import algorithm as alpha_miner
net, initial_marking, final_marking = alpha_miner.apply(data_log['DomesticDeclarations'])

## Import the petrinet visualizer object
from pm4py.visualization.petrinet import visualizer as pn_visualizer
# Visualise 
gviz = pn_visualizer.apply(net, initial_marking, final_marking, variant=pn_visualizer.Variants.FREQUENCY, log=data_log['DomesticDeclarations'])
pn_visualizer.view(gviz)

from pm4py.algo.discovery.inductive import algorithm as inductive_miner
# Discover process tree using inductive miner
tree = inductive_miner.apply_tree(data_log['DomesticDeclarations'])

from pm4py.visualization.process_tree import visualizer as pt_visualizer
gviz = pt_visualizer.apply(tree, parameters={pt_visualizer.Variants.WO_DECORATION.value.Parameters.FORMAT: "png"})
pt_visualizer.view(gviz)

#Adding information about Frequency/Performance


import pm4py

from pm4py.visualization.petri_net import visualizer as pn_visualizer
parameters = {pn_visualizer.Variants.FREQUENCY.value.Parameters.FORMAT: "png"}
gviz = pn_visualizer.apply(net, initial_marking, final_marking, parameters=parameters, variant=pn_visualizer.Variants.FREQUENCY, log=data_log['DomesticDeclarations'])

## Either discover the petri net using inductive miner
#net, initial_marking, final_marking = inductive_miner.apply(log)
## Or convert your existing model
from pm4py.objects.conversion.process_tree import converter as pt_converter
net, initial_marking, final_marking = pt_converter.apply(tree)
## Then visualise
#gviz = pn_visualizer.apply(net, initial_marking, final_marking, variant=pn_visualizer.Variants.FREQUENCY, log=data_log['DomesticDeclarations'])
#pn_visualizer.view(gviz)

pm4py.view_performance_spectrum(data_log['DomesticDeclarations'], ['Declaration SUBMITTED by EMPLOYEE', "Request Payment", "Payment Handled"], format="png")

from pm4py.util import constants
from pm4py.statistics.traces.generic.log import case_statistics
x, y = case_statistics.get_kde_caseduration(data_log['DomesticDeclarations'], parameters={constants.PARAMETER_CONSTANT_TIMESTAMP_KEY: "time:timestamp"})

from pm4py.visualization.graphs import visualizer as graphs_visualizer

gviz = graphs_visualizer.apply_plot(x, y, variant=graphs_visualizer.Variants.CASES)
graphs_visualizer.view(gviz)

gviz = graphs_visualizer.apply_semilogx(x, y, variant=graphs_visualizer.Variants.CASES)
graphs_visualizer.view(gviz)

"""**Detection of Batches**"""

from pm4py.algo.discovery.batches import algorithm 

batches = algorithm.apply(data_log['DomesticDeclarations'])

for act_res in batches:
   print("")
   print("activity: "+act_res[0][0]+" resource: "+act_res[0][1])
   print("number of distinct batches: "+str(act_res[1]))
   for batch_type in act_res[2]:
        print(batch_type, len(act_res[2][batch_type]))

"""**Roles Discovery**"""

roles = pm4py.discover_organizational_roles(data_log['DomesticDeclarations'])
print([x[0] for x in roles])
roles = pm4py.discover_organizational_roles(data_log['InternationalDeclarations'])
print([x[0] for x in roles])

"""**Diagnostics (TBR)**"""

filtered_log = pm4py.filter_variants_top_k(data_log['DomesticDeclarations'], 5)

net, initial_marking, final_marking = pm4py.discover_petri_net_inductive(filtered_log)

from pm4py.algo.conformance.tokenreplay import algorithm as token_based_replay
parameters_tbr = {token_based_replay.Variants.TOKEN_REPLAY.value.Parameters.DISABLE_VARIANTS: True, token_based_replay.Variants.TOKEN_REPLAY.value.Parameters.ENABLE_PLTR_FITNESS: True}
replayed_traces, place_fitness, trans_fitness, unwanted_activities = token_based_replay.apply(data_log['DomesticDeclarations'], net,initial_marking,final_marking,parameters=parameters_tbr)

#Throughput analysis (unfit execution) To perform throughput analysis on the transitions that were executed unfit, and then print on the console the result, the following code could be used:
from pm4py.algo.conformance.tokenreplay.diagnostics import duration_diagnostics
trans_diagnostics = duration_diagnostics.diagnose_from_trans_fitness(data_log['DomesticDeclarations'], trans_fitness)
for trans in trans_diagnostics:
    print(trans, trans_diagnostics[trans])